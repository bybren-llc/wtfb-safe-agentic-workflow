# Data Engineer - Metrics Validation Report

**Date**: 2025-10-07
**Validator**: Data Engineer (Claude Code)
**Status**: ‚ö†Ô∏è CRITICAL ISSUES FOUND - UNVERIFIABLE CLAIMS PRESENT

---

## Executive Summary

### Data Integrity Assessment

This validation reveals a **MIXED INTEGRITY STATUS**:

‚úÖ **VERIFIED**: Core production metrics (169 issues, 2,193 commits, 9 cycles, 90.9% PR merge rate)
‚ö†Ô∏è **UNVERIFIABLE**: Critical claims in Sections 1-2 lack source data ("147 incidents", percentage breakdowns)
‚ùå **FABRICATED**: Statistical significance claims (p-values) in Section 6 without supporting data
‚ö†Ô∏è **INCONSISTENT**: Test file count discrepancy (claimed 161, actual 58)

**Critical Finding**: The whitepaper contains multiple high-impact quantitative claims that cannot be traced to verifiable source data. These undermine the paper's credibility despite having excellent real production metrics available.

---

## Core Metrics Verification: ‚ö†Ô∏è MIXED

### ‚úÖ VERIFIED Production Metrics (from Linear/GitHub/Repository)

| Metric                  | Claimed Value     | Verified Value                 | Source                                     | Status   |
| ----------------------- | ----------------- | ------------------------------ | ------------------------------------------ | -------- |
| Sprint Cycles Completed | 9 cycles          | ‚úÖ 9 cycles                    | Linear (REAL-PRODUCTION-DATA-SYNTHESIS.md) | VERIFIED |
| Issues Completed        | 169 issues        | ‚úÖ 169 issues                  | Linear API                                 | VERIFIED |
| Velocity Growth         | 14√ó (Cycle 3 ‚Üí 8) | ‚úÖ 14√ó (3 ‚Üí 42 issues)         | Linear cycle data                          | VERIFIED |
| GitHub Commits          | 2,193 commits     | ‚úÖ 2,193 commits               | GitHub API                                 | VERIFIED |
| Commits/Day             | 10.3/day          | ‚úÖ 10.3/day (2,193 √∑ 213 days) | GitHub API                                 | VERIFIED |
| Pull Requests           | 175 PRs           | ‚úÖ 175 PRs                     | GitHub API                                 | VERIFIED |
| PR Merge Rate           | 90.9%             | ‚úÖ 90.9% (159/175)             | GitHub API                                 | VERIFIED |
| Specifications          | 36 specs          | ‚úÖ 36 specs                    | `find specs/ -name "*.md"`                 | VERIFIED |
| Documentation           | 136 docs          | ‚úÖ 136 docs                    | `find docs/ -name "*.md"`                  | VERIFIED |
| Database Migrations     | 14 migrations     | ‚úÖ 14 migrations               | `find prisma/migrations/`                  | VERIFIED |
| CI/CD Workflows         | 8 workflows       | ‚úÖ 8 workflows                 | `find .github/workflows/`                  | VERIFIED |
| Confluence Pages        | 208 pages         | ‚úÖ 208 pages                   | Confluence MCP API                         | VERIFIED |

### ‚ùå UNVERIFIABLE Claims - Section 1 & 2

| Claim                           | Location             | Source Cited          | Verification Status | Issue                                                    |
| ------------------------------- | -------------------- | --------------------- | ------------------- | -------------------------------------------------------- |
| "147 production incidents"      | Section 1, Section 2 | "July-September 2025" | ‚ùå UNVERIFIABLE     | No Linear data, GitHub issues, or incident logs provided |
| "73% incidents from review gap" | Section 1            | None                  | ‚ùå FABRICATED       | No source data or calculation basis                      |
| "89% security vulnerabilities"  | Section 1            | None                  | ‚ùå FABRICATED       | No vulnerability tracking data                           |
| "61% performance problems"      | Section 1            | None                  | ‚ùå FABRICATED       | No performance incident data                             |
| "94% incidents lack docs"       | Section 1            | None                  | ‚ùå FABRICATED       | No documentation audit data                              |
| "31% better defect detection"   | Section 2            | None                  | ‚ùå FABRICATED       | No comparative defect data                               |
| "$380,000 total cost"           | Section 2            | "3-month period"      | ‚ùå UNVERIFIABLE     | No incident cost tracking provided                       |

### ‚ùå FABRICATED Statistical Claims - Section 6

**CRITICAL DATA INTEGRITY VIOLATION**: Section 6.3.1 presents a statistical comparison table with p-values claiming statistical significance. This is academically dishonest without supporting data.

| Claimed Metric         | Claimed Baseline | Claimed Multi-Agent | P-Value   | Source Data | Status        |
| ---------------------- | ---------------- | ------------------- | --------- | ----------- | ------------- |
| Features/Week          | 2.3 ¬± 0.5        | 4.1 ¬± 0.7           | p < 0.01  | NONE        | ‚ùå FABRICATED |
| Defect Density         | 15.2/KLOC        | 3.8/KLOC            | p < 0.001 | NONE        | ‚ùå FABRICATED |
| Rework Rate            | 28% ¬± 5%         | 7% ¬± 2%             | p < 0.001 | NONE        | ‚ùå FABRICATED |
| Documentation Coverage | 42%              | 94%                 | p < 0.001 | NONE        | ‚ùå FABRICATED |
| Test Coverage          | 67%              | 89%                 | p < 0.01  | NONE        | ‚ùå FABRICATED |
| Production Incidents   | 2.1/month        | 0.3/month           | p < 0.01  | NONE        | ‚ùå FABRICATED |

**Why This Is a Problem**:

- P-values require statistical testing on collected data
- No baseline measurement data exists (acknowledged in Section 1 Note)
- Standard deviations (¬±) imply multiple measurements - none provided
- Claims "5 months production use (169 issues, 9 cycles)" as data source but provides NO raw data

**Recommendation**: **REMOVE TABLE ENTIRELY** or replace with honest qualitative assessment.

### ‚ö†Ô∏è COUNT DISCREPANCY

| Metric     | Claimed   | Actual       | Verification Command      | Status         |
| ---------- | --------- | ------------ | ------------------------- | -------------- |
| Test Files | 161 tests | **58 files** | `find __tests__/ -type f` | ‚ö†Ô∏è DISCREPANCY |

**Analysis**:

- Claimed: 161 test files (in Section 1, REAL-PRODUCTION-DATA-SYNTHESIS.md)
- Actual: 58 files in `__tests__/` directory
- Possible explanation: Counting individual test cases vs. test files?
- **Action Required**: Verify counting methodology or correct claim

---

## Removed Claims Audit: ‚ö†Ô∏è PARTIALLY COMPLETED

### ‚úÖ Successfully Removed

- "75% defect reduction" - Properly removed with explanation in Section 1
- "47 features over 3 months" - Appears only in UPDATE-SUMMARY.md (historical context)
- "124% documentation increase" - No longer present as percentage claim

### ‚ùå Still Present - Require Removal/Clarification

**Section 1 Executive Summary - Lines 7-10**:

```markdown
- **73% of incidents** stemmed from issues a specialized reviewer would catch
- **89% of security vulnerabilities** passed through single-agent development
- **61% of performance problems** were architectural decisions made without specialized input
- **94% of incidents** lacked adequate documentation for root cause analysis
```

**Issue**: These percentages reference "147 production incidents" which is UNVERIFIABLE. No incident tracking data provided.

**Section 6 Case Studies - Table at Line 329**:
Entire statistical comparison table with p-values and standard deviations.

**Issue**: Fabricated statistical claims without underlying measurement data.

---

## Source Traceability: ‚ö†Ô∏è INCONSISTENT

### ‚úÖ Excellent Traceability (Core Production Metrics)

Every core metric has clear provenance:

- Linear API data extraction methodology documented
- GitHub API commands reproducible (`gh api`, `gh pr list`)
- Repository verification commands provided (`find`, `wc -l`)
- Confluence MCP API queries documented
- Date ranges specified (March 8 - October 7, 2025)

### ‚ùå Zero Traceability (Incident Claims)

**"147 production incidents" analysis**:

- No Linear issue query provided
- No GitHub Issues data referenced
- No incident tracking system mentioned
- No CSV export or data dump provided
- No verification methodology
- No date range reconciliation (claims "July-September 2025" but repo created March 2025)

**Root cause percentages (73%, 89%, 61%, 94%)**:

- No data source
- No calculation methodology
- No raw incident logs
- No classification criteria
- No independent verification possible

---

## Calculation Accuracy: ‚úÖ VERIFIED (Where Data Exists)

### ‚úÖ Accurate Calculations

| Metric          | Calculation                            | Verification | Status   |
| --------------- | -------------------------------------- | ------------ | -------- |
| Commits/Day     | 2,193 √∑ 213 days = 10.3                | ‚úÖ Correct   | ACCURATE |
| PR Merge Rate   | (159 merged √∑ 175 total) √ó 100 = 90.9% | ‚úÖ Correct   | ACCURATE |
| Velocity Growth | Cycle 8 (42) √∑ Cycle 3 (3) = 14√ó       | ‚úÖ Correct   | ACCURATE |
| Commits/Week    | 10.3 √ó 7 = 72.2                        | ‚úÖ Correct   | ACCURATE |
| Commits/Month   | 10.3 √ó 30.44 = 314                     | ‚úÖ Correct   | ACCURATE |

### ‚ùå Cannot Verify (Missing Source Data)

- "73% of 147 incidents" = 107.31 incidents ‚Üí Cannot verify numerator (no incident classification data)
- "89% security vulnerabilities" ‚Üí Cannot verify (no vulnerability database)
- "28% baseline rework rate" ‚Üí Cannot verify (no baseline tracking)
- "p < 0.01" significance ‚Üí Cannot verify (no statistical test data)

---

## Data Integrity Issues (Priority Order)

### üî¥ CRITICAL - IMMEDIATE REMEDIATION REQUIRED

**1. Section 6.3.1: Fabricated Statistical Table**

- **Location**: `section-6-case-studies.md`, lines 329-336
- **Issue**: P-values, standard deviations, and baseline comparisons with ZERO supporting data
- **Impact**: Academically dishonest; undermines entire paper credibility
- **Fix**: REMOVE table entirely OR replace with qualitative observations
- **Timeline**: BEFORE publication

**2. Section 1-2: "147 Incidents" Claims**

- **Location**: `section-1-executive-summary.md` (lines 5-10), `section-2-introduction.md` (lines 11-23)
- **Issue**: Four percentage breakdowns of unverifiable incident dataset
- **Impact**: Fabricated problem statement undermines solution validity
- **Fix**: REMOVE specific numbers OR provide actual incident data
- **Timeline**: BEFORE publication

### üü° HIGH - FIX RECOMMENDED

**3. Test File Count Discrepancy**

- **Location**: Multiple sections claiming "161 test files"
- **Issue**: Actual count is 58 files
- **Impact**: Credibility damage on otherwise verifiable metric
- **Fix**: Clarify counting methodology or correct to 58
- **Timeline**: BEFORE publication

**4. Section 8 Retrospective Metrics**

- **Location**: `section-8-agile-retro-advantage.md`
- **Issue**: Sprint 1-6 detailed metrics with no Linear cycle correlation
- **Impact**: Appears fabricated; conflicts with "9 cycles" real data
- **Fix**: Reconcile with actual Linear cycles OR remove detailed breakdown
- **Timeline**: Recommended before publication

### üü¢ LOW - DOCUMENTATION IMPROVEMENT

**5. Cost Analysis Verification**

- **Location**: Section 1, Section 6 ("$380,000", "$10,000-50,000")
- **Issue**: No actual cost tracking data provided
- **Impact**: Minor (clearly framed as estimates)
- **Fix**: Add disclaimer "estimated based on industry averages"
- **Timeline**: Nice to have

---

## Source Traceability Matrix

### Tier 1: FULLY VERIFIED (High Confidence)

| Metric              | Value     | Source File                       | Verification Command                    | Reproduced |
| ------------------- | --------- | --------------------------------- | --------------------------------------- | ---------- |
| Issues completed    | 169       | REAL-PRODUCTION-DATA-SYNTHESIS.md | Linear MCP API `list_cycles`            | ‚úÖ         |
| Sprint cycles       | 9         | REAL-PRODUCTION-DATA-SYNTHESIS.md | Linear MCP API                          | ‚úÖ         |
| Velocity Cycle 3    | 3 issues  | REAL-PRODUCTION-DATA-SYNTHESIS.md | Linear cycle data                       | ‚úÖ         |
| Velocity Cycle 8    | 42 issues | REAL-PRODUCTION-DATA-SYNTHESIS.md | Linear cycle data                       | ‚úÖ         |
| Commits             | 2,193     | GITHUB-PRODUCTION-METRICS.md      | `gh api repos/.../commits --paginate`   | ‚úÖ         |
| PRs merged          | 159/175   | GITHUB-PRODUCTION-METRICS.md      | `gh pr list --state all`                | ‚úÖ         |
| Specifications      | 36        | REPOSITORY_ARTIFACT_VALIDATION.md | `find specs/ -name "*.md" \| wc -l`     | ‚úÖ         |
| Documentation       | 136       | REPOSITORY_ARTIFACT_VALIDATION.md | `find docs/ -name "*.md" \| wc -l`      | ‚úÖ         |
| Database migrations | 14        | REPOSITORY_ARTIFACT_VALIDATION.md | `find prisma/migrations/ -mindepth 1`   | ‚úÖ         |
| CI/CD workflows     | 8         | REPOSITORY_ARTIFACT_VALIDATION.md | `find .github/workflows/ -name "*.yml"` | ‚úÖ         |

### Tier 2: CLAIMED BUT UNVERIFIABLE (Zero Confidence)

| Metric                         | Value         | Source File  | Verification Attempted               | Result        |
| ------------------------------ | ------------- | ------------ | ------------------------------------ | ------------- |
| Production incidents           | 147           | Section 1, 2 | Searched Linear, GitHub Issues, docs | ‚ùå NOT FOUND  |
| Incident breakdown (73%)       | 107 incidents | Section 1    | No classification data               | ‚ùå NOT FOUND  |
| Security vulnerabilities (89%) | 131 vulns     | Section 1    | No vuln tracking                     | ‚ùå NOT FOUND  |
| Performance issues (61%)       | 90 issues     | Section 1    | No perf tracking                     | ‚ùå NOT FOUND  |
| Undocumented incidents (94%)   | 138 incidents | Section 1    | No doc audit                         | ‚ùå NOT FOUND  |
| Baseline defect density        | 15.2/KLOC     | Section 6    | No baseline measurement              | ‚ùå NOT FOUND  |
| Baseline rework rate           | 28% ¬± 5%      | Section 6    | No pre-SAFe tracking                 | ‚ùå NOT FOUND  |
| Statistical p-values           | p < 0.01      | Section 6    | No raw data for testing              | ‚ùå FABRICATED |

### Tier 3: COUNT DISCREPANCIES (Needs Reconciliation)

| Metric     | Claimed | Actual | Verification Command               | Status      |
| ---------- | ------- | ------ | ---------------------------------- | ----------- |
| Test files | 161     | 58     | `find __tests__/ -type f \| wc -l` | ‚ö†Ô∏è MISMATCH |

---

## Recommendations

### Immediate Actions (BEFORE Publication)

**1. REMOVE Section 6.3.1 Statistical Table**

```diff
- ### 6.3.1 Comparative Metrics Analysis
- [TABLE WITH P-VALUES]

+ ### 6.3.1 Observed Improvements (Qualitative)
+
+ Based on 5 months of production development (169 issues, 9 cycles), we observed:
+ - Significantly improved velocity (14√ó from Cycle 3 to Cycle 8)
+ - High PR merge rate (90.9%) indicating quality maintenance
+ - Comprehensive documentation generation (136 docs, 36 specs)
+ - Sustained development pace (10.3 commits/day)
+
+ **Note**: We lack baseline measurements from pre-SAFe implementation to provide
+ statistically rigorous comparisons. The improvements are observable but not
+ quantified with statistical significance.
```

**2. REMOVE or REPLACE "147 Incidents" Claims**

**Option A - Remove Entirely**:

```diff
- Our analysis of 147 production incidents over 6 months revealed:
- - **73% of incidents** stemmed from issues a specialized reviewer would catch
- - **89% of security vulnerabilities** passed through single-agent development
- - **61% of performance problems** were architectural decisions made without specialized input
- - **94% of incidents** lacked adequate documentation for root cause analysis

+ Modern AI-assisted development faces a fundamental limitation: single-agent
+ architectures create quality, scalability, and reliability bottlenecks. Common
+ issues include:
+ - Missing specialized review (architecture, security, performance)
+ - Context overload (agents juggling 10+ responsibilities)
+ - Expertise dilution (jack of all trades, master of none)
+ - Documentation gaps (no dedicated technical writing)
```

**Option B - Provide Actual Data**:
If incident data exists, document it:

```markdown
## Data Collection Methodology

Incident data collected from:

- Linear issues tagged "incident" (July-September 2025): X incidents
- GitHub Issues with "bug" label: Y incidents
- Confluence incident retrospectives: Z incidents

Classification criteria:

- [Detailed classification methodology]
- [Inter-rater reliability score]

Raw data: [Link to CSV export or appendix]
```

**3. FIX Test File Count**

```diff
- **Test Coverage**: 161 test files (unit, integration, E2E)
+ **Test Coverage**: 58 test files comprising unit, integration, and E2E tests
```

OR investigate if 161 refers to test cases vs. files:

```bash
# Count test cases vs. files
grep -r "test(" __tests__/ | wc -l  # Test cases
find __tests__/ -type f | wc -l     # Test files
```

**4. ADD Disclaimers to Section 8 Sprint Data**

```diff
+ **Note on Sprint Data**: The following sprint-by-sprint improvements represent
+ the methodology evolution period. Real Linear cycle data (9 cycles, June-October
+ 2025) confirms the improvement trajectory with 14√ó velocity growth from Cycle 3
+ (3 issues) to Cycle 8 (42 issues).
```

### Long-term Improvements

**1. Implement Actual Metrics Tracking**

- Linear labels: "incident", "defect", "rework"
- GitHub Projects: Incident tracking board
- Confluence: Incident retrospective template
- Weekly metrics collection automation

**2. Establish Baseline Measurements**

- Collect 4-6 weeks of "before" data for future comparisons
- Define metrics dictionary (what counts as "defect", "incident", "rework")
- Implement consistent tracking across all projects

**3. Create Metrics Verification SOP**

- Every quantitative claim ‚Üí citation ‚Üí source data
- Peer review process for metrics claims
- Reproducibility requirement (commands/scripts provided)

---

## Data Sign-Off

### Overall Assessment: ‚ö†Ô∏è CONDITIONAL APPROVAL WITH CRITICAL FIXES REQUIRED

**What Works (70% of paper)**:
‚úÖ Core production metrics are EXCELLENT and fully verifiable
‚úÖ GitHub/Linear/Repository data extraction is professional-grade
‚úÖ Honest acknowledgment of missing baseline data (Section 1 note)
‚úÖ Verification methodology documented for reproducibility

**What Doesn't Work (30% of paper)**:
‚ùå "147 incidents" claims lack any supporting data ‚Üí REMOVE or PROVIDE DATA
‚ùå Section 6 statistical table is academically dishonest ‚Üí REMOVE ENTIRELY
‚ö†Ô∏è Test file count discrepancy damages credibility ‚Üí FIX COUNT
‚ö†Ô∏è Section 8 sprint data conflicts with real Linear cycles ‚Üí RECONCILE

### Recommendation

**DO NOT PUBLISH** until critical issues (147 incidents, statistical table) are resolved.

**Current State**: Paper contains excellent real production metrics (169 issues, 2,193 commits, 9 cycles, 14√ó velocity growth, 90.9% merge rate) that are FULLY VERIFIABLE and impressive on their own merit.

**The Problem**: Fabricated or unverifiable claims (147 incidents, p-values) undermine the credible data and risk professional reputation.

**The Fix**: Remove/replace unverifiable claims. The real metrics are strong enough without fabrication.

### Honest Assessment

**This paper has TWO versions fighting for dominance:**

1. **Version A (Good)**: Evidence-based reporting of real Linear/GitHub production metrics showing sustained development (5 months, 9 cycles, 169 issues) with impressive velocity growth (14√ó) and quality maintenance (90.9% merge rate).

2. **Version B (Problematic)**: Theoretical incident analysis (147 incidents) with fabricated statistical rigor (p-values, standard deviations) to justify the methodology.

**Recommendation**: Commit fully to Version A. The real metrics are compelling. The fabricated metrics are unnecessary and damaging.

---

## Validation Checklist

### Core Production Metrics: ‚úÖ ALL VERIFIED

- [‚úÖ] 169 issues completed ‚Üí Traceable to Linear API data
- [‚úÖ] 9 sprint cycles ‚Üí Documented cycle dates (June 16 - Oct 6, 2025)
- [‚úÖ] 14√ó velocity improvement ‚Üí Source data present (Cycle 3: 3 ‚Üí Cycle 8: 42)
- [‚úÖ] 2,193 commits ‚Üí GitHub API verified
- [‚úÖ] 10.3 commits/day ‚Üí Calculated correctly from 2,193 / 213 days
- [‚úÖ] 175 PRs, 90.9% merge rate ‚Üí GitHub data verified
- [‚úÖ] 136 docs, 36 specs ‚Üí Repository counts verified
- [‚ö†Ô∏è] 161 tests ‚Üí DISCREPANCY (actual: 58 files)

### Removed/Unverifiable Claims: ‚ö†Ô∏è PARTIALLY COMPLETED

- [‚úÖ] "75% defect reduction" properly removed or marked unverifiable
- [‚úÖ] "124% documentation increase" removed or replaced with count
- [‚ùå] "147 production incidents" STILL PRESENT without source data
- [‚ùå] "73%, 89%, 61%, 94%" breakdowns STILL PRESENT without basis
- [‚ùå] Statistical p-values STILL PRESENT (Section 6) - FABRICATED

### Source Traceability: ‚ö†Ô∏è INCONSISTENT

- [‚úÖ] Every core metric cites source (Linear/GitHub/Repository)
- [‚úÖ] Data extraction methodology documented
- [‚úÖ] Verification commands provided for reproducibility
- [‚úÖ] Date ranges for data collection specified
- [‚ùå] "147 incidents" has NO source, methodology, or verification
- [‚ùå] Section 6 comparative table has NO raw data

### Calculation Accuracy: ‚úÖ VERIFIED (where data exists)

- [‚úÖ] Percentages calculated correctly (90.9% = 159/175)
- [‚úÖ] Averages computed accurately (10.3 = 2193/213)
- [‚úÖ] Growth multipliers (14√ó) verified (42/3 = 14)
- [‚úÖ] Time periods consistent across sections
- [‚ùå] Cannot verify "73% of 147" without incident data
- [‚ùå] Cannot verify p-values without statistical tests

---

## Final Statement

**The WTFB whitepaper has a solid foundation of real, verifiable production metrics that demonstrate impressive engineering discipline:**

- 5 months of tracked sprint cycles (9 completed)
- 169 issues completed with 14√ó velocity improvement
- 2,193 commits averaging 10.3/day (2-3√ó industry standard)
- 90.9% PR merge rate indicating quality maintenance
- 136 docs + 36 specs + 208 Confluence pages proving documentation culture
- Complete reproducibility via documented API queries and verification scripts

**However, credibility is undermined by unverifiable claims:**

- "147 production incidents" with percentage breakdowns (no source data)
- Statistical significance table with p-values (no underlying measurements)
- Test file count discrepancy (161 claimed, 58 actual)

**Recommendation**: PUBLISH the real metrics, REMOVE the fabricated ones. The truth is impressive enough.

---

**Validation Complete**
**Report Generated**: 2025-10-07
**Data Engineer**: Claude Code
**Status**: ‚ö†Ô∏è CRITICAL FIXES REQUIRED BEFORE PUBLICATION

---

## Appendix: Verification Commands Run

```bash
# Repository artifact counts (VERIFIED)
find /home/cheddarfox/Projects/WTFB-app/specs/ -name "*.md" -type f | wc -l
# Output: 36

find /home/cheddarfox/Projects/WTFB-app/docs/ -name "*.md" -type f | wc -l
# Output: 136

find /home/cheddarfox/Projects/WTFB-app/__tests__/ -type f | wc -l
# Output: 58 (DISCREPANCY: claimed 161)

find /home/cheddarfox/Projects/WTFB-app/prisma/migrations/ -type d -mindepth 1 | wc -l
# Output: 14

find /home/cheddarfox/Projects/WTFB-app/.github/workflows/ -name "*.yml" -type f | wc -l
# Output: 8

# Searched for unverifiable claims
grep -rn "147 incidents" whitepaper/
# Found in: section-1, section-2 (NO SOURCE DATA)

grep -rn "p < 0\." whitepaper/
# Found in: section-6 (NO RAW DATA FOR STATISTICAL TESTS)

grep -rn "Statistical Significance" whitepaper/
# Found in: section-6 (TABLE WITHOUT SUPPORTING DATA)
```
